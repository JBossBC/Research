# linux network virtual

linux目前提供的八种名称空间里，网络名称空间无疑是隔离内容最多的一种，它为名称空间内所有进程提供了全套的网络设施，包括独立的设备界面、路由表、ARP表、IP地址表、iptables/ebtables规则、协议栈等等。虚拟化容器是以linux名称空间的隔离性为基础来实现的,那解决隔离的容器之间、容器与宿主机之间、乃至跨物理网络的不同容器间通信问题的责任，很自然也落在了 Linux 网络虚拟化技术的肩上。


## 网络通信模型

整体来看，linux系统的通信过程无论是按照理论上的OSI七层模型，还是以实际上的TCP/IP四层模型来解构，都明显地呈现出"逐层调用，逐层封装"的特点，这种逐层处理的方式与栈结构类似，因此它通常被称为linux网络协议栈。

网络栈(应用层以下)都位于系统内核之中，采用这种设计，主要是从数据安全隔离的角度出发考虑的。由内核去处理网络报文的收发，无疑会有更高的执行开销，譬如数据在内核态和用户态之间来回拷贝的额外成本，因此会损失一些性能，但是能够保证应用程序无法窃听或者去伪造另一个应用程序的通信内容。针对特别关注收发性能的应用场景，也有直接在用户空间中实现全套协议栈的旁路方案，譬如开源的Netmap以及Intel的DPDK,都能做到零拷贝收发网络数据包。

+ socket: 应用层的程序是通过socket编程接口来和内核空间的网络协议栈通信的。linux socket是从BSD socket发展而来的，现在socket已经不局限于某个操作系统的专属功能，成为各大主流操作系统共同支持的通用网络编程接口,是网络应用程序实际上的交互基础。应用程序通过读写收、发缓冲区来与socket进行交互，在unix和linux系统中，处于一切皆是文件的设计哲学，对socket操作被实现为对文件系统的读写访问操作,通过文件描述符来进行。
+ TCP/UDP:传输层协议族里面最重要的协议就是传输控制协议和用户数据报协议两种，它们是在linux内核中被直接支持的协议，此外还有流控制传输协议(SCTP)、数据报拥塞控制协议(DCCP)等等。不同的协议处理流程大致是一样的，只是把封装的报文以及头、尾部信息会有所不同，这里以 TCP 协议为例，内核发现 Socket 的发送缓冲区中有新的数据被拷贝进来后，会把数据封装为 TCP Segment 报文，常见网络协议的报文基本上都是由报文头（Header）和报文体（Body，也叫荷载“Payload”）两部分组成。系统内核将缓冲区中用户要发送出去的数据作为报文体，然后把传输层中的必要控制信息，譬如代表哪个程序发、由哪个程序收的源、目标端口号，用于保证可靠通信（重发与控制顺序）的序列号、用于校验信息是否在传输中出现损失的校验和（Check Sum）等信息封装入报文头中。
+ IP:网络层协议最主要的就是网际协议(IP),其他还有因特网组管理协议(IGMP)、大量的路由协议(EGP、NHRP、OSRF、IGRP)等等。
+ Device:网络设备是网络访问层中面向系统一侧的接口，这里所说的设备与物理硬件设备并不是同一个概念，Device只是一种向操作系统端开放的接口，其背后既可能是真实的物理硬件，也有可能是某段具有特定功能的程序代码,譬如即使不存在物理网卡，也依然可以存在回环设备。许多网络抓包工具便是在此处工作的。Device 主要的作用是抽象出统一的界面，让程序代码去选择或影响收发包出入口，譬如决定数据应该从哪块网卡设备发送出去；还有就是准备好网卡驱动工作所需的数据，譬如来自上一层的 IP 数据包、下一跳 （Next Hop）的 MAC 地址（这个地址是通过ARP Request 得到的）等等。
+ Driver:网卡驱动程序是网络访问层中面向硬件一侧的接口，网卡驱动程序会通过DMA将主存中的待发送数据包复制到驱动内部的缓冲区之内。数据被复制的同时，也会将上层提供的IP数据包、下一条MAC地址这些信息，加上网卡的MAC地址、VLAN Tag等信息一并封装成为以太帧,并自动计算校验和。对于需要确认重发的信息，如果没有收到接收者的确认响应，重发的处理也是在这里自动完成的。


## 干预网络通信

网络协议栈的处理是一套相对固定和封闭的流程，整套处理过程中，除了在网络设备这层能看到一点点程序以设备的形式介入处理的空间外，其他过程似乎就没有什么可供程序插手的余地了。然而在linux kernel 2.4版开始，内核开放了一套通用的、可供代码干预数据在协议栈中流转的过滤器框架。这套名为Netfilter框架是linux防火墙和网络的主要维护者提出的，它围绕网络层(IP协议)的周围，埋下了五个钩子,每当有数据包流到网络层，经过这层钩子时，就会自动出发由内核模块注册在这里的回调函数，程序代码就能够通过回调来干预linux网络通信。

+ PREROUTING:来自设备的数据包进入协议栈后立即触发此钩子。PREROUTING钩子在进入IP路由之前触发，这意味着只要接收到的数据包，无论是否真的发往本机，都会触发此钩子。一般用于目标网络地址转换。
+ INPUT:报文经过IP路由后，如果确定是发往本机的，就会触发此钩子，一般用于加工发往本地进程的数据包
+ FORWARD:报文经过IP路由后，如果确定不是发往本机的，就会触发此钩子，一般用于处理转发到其他机器的数据包
+ OUTPUT: 从本机程序发出的数据包。在经过IP路由前，就会触发此钩子，一般用于加工本地进程的输出数据包
+ POSTROUTING:从本机网卡出去的数据包,无论是本机的程序发出的，还是由本机转发给其他机器的，都会触发此钩子，一般用于源网络地址转换。

Netfilter允许在同一钩子处注册多个回调函数，因此向钩子注册回调函数时必须提供明确的优先级，以便触发时能按照优先级从高到低进行激活。由于回调函数会存在很多个，看起来就像挂在同一个钩子的一串链条，因此钩子触发的回调函数集合就被称为"回调链"。这个名字导致了后续基于 Netfilter 设计的 Xtables 系工具，如稍后介绍的 iptables 均有使用到“链”（Chain）的概念。**虽然现在看来 Netfilter 只是一些简单的事件回调机制而已，然而这样一套简单的设计，却成为了整座 Linux 网络大厦的核心基石，Linux 系统提供的许多网络能力，如数据包过滤、封包处理（设置标志位、修改 TTL 等）、地址伪装、网络地址转换、透明代理、访问控制、基于协议类型的连接跟踪，带宽限速，等等，都是在 Netfilter 基础之上实现的。**



iptables被称为linux系统的自带防火墙，然而iptables实际能做的事情已经远远超出防火墙的范畴。**iptables的设计意图是因为Netfilter的钩子回调虽然很强大，但是需要通过程序编码才能够使用，并不适合系统管理员日常运维，而它的价值便是以配置去实现原本用NetFilter编码才能做到的事情。iptables先把用户常用的管理意图总结成具体的行为预先准备好，然后在满足条件时自动激活行为。**



这些行为本来能够被挂载到Netfilter钩子的回调链上，但iptables又进行了一层额外抽象，不是把行为与链直接挂钩，而是根据这些底层操作的目的，先总结出更高层次的规则。举个例子，假设你挂载规则目的是为了实现网络地址转换（NAT），那就应该对符合某种特征的流量（譬如来源于某个网段、从某张网卡发送出去）、在某个钩子上（譬如做 SNAT 通常在 POSTROUTING，做 DNAT 通常在 PREROUTING）进行 MASQUERADE 行为，这样具有相同目的的规则，就应该放到一起才便于管理，由此便形成“规则表”的概念。iptables 内置了五张不可扩展的规则表（其中 security 表并不常用，很多资料只计算了前四张表），如下所列：

1. raw表:用于去除数据包上的连接追踪机制
2. mangle表:用于修改数据包的报文头信息，如服务类型、生存周期以及为数据包设置Mark标记，典型的应用是链路的服务质量管理
3. nat表，用于修改数据包的源或者目的地址等信息，典型的应用是网络地址转换
4. filter表:用于对数据包进行过滤，控制到达某条链上的数据包是继续放行还是直接丢弃或拒绝，典型的应用是防火墙
5. security表: 用于在数据包上应用SELinux，这张表并不常用

iptables 不仅仅是 Linux 系统自带的一个网络工具，它在容器间通信中扮演相当重要的角色，譬如 Kubernetes 用来管理 Service 的 Endpoints 的核心组件 kube-proxy，就依赖 iptables 来完成 ClusterIP 到 Pod 的通信（也可以采用 IPVS，IPVS 同样是基于 Netfilter 的），这种通信的本质就是一种 NAT 访问。



## 虚拟化网络设备


虚拟化网络并不需要完全遵照物理网络的样子来设计，不过，由于已有大量现成的代码本来就是面向于物理存在的网络设备来编码实现的，也有处于方便理解和知识继承的方面的考虑，虚拟化网络与物理网络中的设备还是有相当高的相似性。

## 虚拟网卡: tun/tap


目前主流的虚拟网卡方案有tun/tap和veth两种，在时间上tun/tap出现得更早，它是一组通用的虚拟驱动程序包，里面包含了两个设备，分别是用于网络数据包处理的虚拟网卡驱动，以及用于内核空间与用户空间交互的字符设备(这里指的是/dev/net/tun)驱动。大概在2000年左右，solaris系统为了实现隧道协议开发了这套驱动,从linux kernel2.1版开始移植到linux内核中，当时是源码中的可选模块，2.4版之后发布的内核都会默认编译tun/tap驱动。

tun和tap是两个相对独立的虚拟网络设备，其中tap模拟了以太网设备，操作二层数据包(以太帧)、tun则模拟了网络层设备，操作三层数据包。**使用tun/tap设备的目的是实现把来自协议栈的数据包先交由某个打开了/dev/net/tun字符设备的用户进程处理后,再把数据包重新发回到链路中。**可以通俗地将它理解为**这块虚拟化网卡驱动一端连接着网络协议栈，另一端连接着用户态程序，而普通的网卡驱动则是一端连接着网络协议栈，另一端连接着物理网卡**。只要协议栈中地数据包能被用户态程序截获并加工处理,程序员就有足够的舞台空间去玩出各种花样，譬如数据压缩，流量加密，透明代理等功能都可以以此为基础来实现。

应用程序通过tun设备对外发送数据包后，tun设备如果发现另一端的字符设备已经被VPN程序打开(这就是一端连接着网络协议栈，另一端连接着用户态程序)，便会把数据包通过字符设备发送给VPN程序，VPN收到数据包，会修改后再重新封装为新报文，譬如数据包原本是发送给A地址的，VPN把整个包进行加密，然后作为报文体，封装到另一个发送给B地址的新数据包当中。这种将一个数据包套进另一个数据包中的处理方式被形象称为隧道(Tunneling),隧道技术是再物理网络中构筑逻辑网络的经典做法。而其中提到的加密，也有标准的协议可遵循，譬如IPSec协议。


使用tun/tap设备传输数据需要经过两次协议栈，不可避免地会有一定的性能损耗，如果条件允许,容器对容器的直接通信并不会把tun/tap作为首选方案，一般是基于veth来实现的。但是tun/tap没有veth那样要求设备成对出现、数据要原样传输的限制，数据包到用户态进程后，程序员就有完全掌控的权力，要进行哪些修改，要发送到什么地方，都可以编写代码去实现，因此 tun/tap 方案比起 veth 方案有更广泛的适用范围。


## 虚拟网卡: veth

veth 是另一种主流的虚拟网卡方案，在 Linux Kernel 2.6 版本，Linux 开始支持网络名空间隔离的同时，也提供了专门的虚拟以太网（Virtual Ethernet，习惯简写做 veth）让两个隔离的网络名称空间之间可以互相通信。直接把 veth 比喻成是虚拟网卡其实并不十分准确，如果要和物理设备类比，它应该相当于由交叉网线 连接的一对物理网卡。


veth实际上不是一个设备，而是一对设备，因此也常被称作veth pair。要使用veth，必须在两个独立的网络名称空间中才有意义，因为veth pair是一端连接协议栈，另一端彼此相连的，在veth设备的其中一端输入数据，这些数据就会从设备的另一端原样不变地流出

由于两个容器之间采用 veth 通信不需要反复多次经过网络协议栈，这让 veth 比起 tap/tun 具有更好的性能，也让 veth pair 的实现变的十分简单，内核中只用了几十行代码实现了一个数据复制函数就完成了 veth 的主体功能。veth 以模拟网卡直连的方式很好地解决了两个容器之间的通信问题，然而对多个容器间通信，如果仍然单纯只用 veth pair 的话，事情就会变得非常麻烦，让每个容器都为与它通信的其他容器建立一对专用的 veth pair 并不实际，这时就迫切需要有一台虚拟化的交换机来解决多容器之间的通信问题了。


## 交换机 linux bridge

既然有了虚拟网卡，很自然也联想到让网卡接入到交换机里，实现多个容器间的相互连接。linux bridge便是linux系统下虚拟化交换机。虽然他以网桥而不是交换机命名，然后使用过程中，linux bridge的目的看起来就像交换机,功能使用起来像交换机、程序实现起来也像交换机，实际就是一台虚拟交换机。

linux bridge是在linux kernel 202版本开始提供的二层转发工具，由brctl命令创建和管理。linux bridge创建之后，便能够接入任何位于二层的网络设备，无论是真实的物理设备或者是虚拟的设备，都能与linux bridge配合工作，当有二层数据包(以太帧)从网卡进入linux bridge，它将根据数据包的类型和目标MAC地址，按如下规则转发处理:


+ 如果数据包是广播帧，转发给所有接入网桥的设备
+ 如果数据包是单播帧:

       + 且MAC地址在地址转发表中不存在，则泛洪给所有接入网桥的设备，并将响应设备的接口与MAC地址学习，到自己的MAC地址转发表中。
       + 且MAC地址在转发表中已经存在，则直接转发到地址表中指定的设备
+ 如果数据包是此前转发过的，又会重新发挥bridge，说明冗余链路产生了回环。由于以太帧不像IP报文那样有TTL来约束，因此一旦出现环路，如果没有额外措施来处理的话就会永不停歇地转发下去。对于这种数据包就需要交换机实现生成树协议(STP)来交换拓扑信息，生成唯一拓扑链路以切断环路。


Linux Bridge 不仅用起来像交换机，实现起来也像交换机。不过，它与普通的物理交换机也还是有一点差别的，普通交换机只会单纯地做二层转发，Linux Bridge 却还支持把发给它自身的数据包接入到主机的三层的协议栈中。


对于通过brctl命令显式接入网桥地设备，linux bridge与物理交换机的转发行为是完全一致的，也不允许给接入的设备设置IP地址，因为网桥是根据MAC地址做二层转发的，就算设置了三层的IP地址也毫无意义。然而linux bridge与普通交换机的区别是除了显式接入的设备外，它自己也无可分割地连接着一台有着完整网络协议栈地linux主机，因为linux bridge本身肯定是在某台linux主机上创建的，可以看作linux bridge有一个与自己名字相同的隐藏端口，隐式地连接了创建它的那台linux主机。因此，linux bridge允许给自己设置ip地址，比普通交换机多一种特殊的转发情况:

+ 如果数据包的目的MAC地址为网桥本身，并且网桥有设置了IP地址的话，那该数据包即被认为是收到发往创建网桥那台主机的数据包，此数据包将不会转发到任何设备，而是直接交给上层协议栈去处理。


此时，网桥就取代了 eth0 设备来对接协议栈，进行三层协议的处理。设置这条特殊转发规则的好处是：只要通过简单的 NAT 转换，就可以实现一个最原始的单 IP 容器网络。


## 网络: vxlan

有了虚拟化网络设备后，下一步就是要使用这些设备组成网络，容器分布在不同的物理主机上，每一台物理主机都有物理网络相互联通，然而这些网络的物理拓扑结构是相对固定的，很难跟上云原生时代的分布式系统的逻辑拓扑结构变动频率，譬如服务的扩缩、断路、限流等，都可能要求网络跟随做出相应的变化。正因如此，软件定义网络的需求在云计算和分布式时代变得前所未有地迫切，SDN的核心思路是在物理的网络之上再构造一层虚拟化的网络，将控制平面和数据平面分离开来，实现流量灵活控制，为核心网络及应用的创新提供良好的平台。SDN里位于下层的物理网络被称为underlay,它着重解决网络的连通性与可管理型，位于上层的逻辑网络被称为overlay，它着重为应用提供与软件需求相符的传输服务和网络拓扑。

软件定义网络已经发展了十余年时间，远比云原生、微服务这些概念出现得更早。网络设备商基于软件设备开发出了EVI(Ethernet virtualization interconnect)、TRILL(Transparent interconnection of lots of links)、SPB(Shortest Path Bridging)等大二层网络技术;软件产商也提出VXLAN(virtual eXtensible LAN)、NAGRE(Network virtualization using generic routing Encapsulation)、STT等一系列基于虚拟交换机实现的Overlay网络。跨主机的容器间通信，用的大多都是overlay网络。

VLAN的全称是虚拟局域网,由于二层网络本身的工作特性决定了它非常依赖于广播，无论是广播帧(ARP请求、DHCP、RIP都会产生广播帧)，还是泛洪路由，其执行成本都随着接入二层网络设备数量的增长而等比增长，当设备太多，广播又频繁的时候，很容易就会形成广播风暴。因此，VLAN的首要职责就是划分广播域，将连接在同一个物理网络上的设备区分开来，划分的具体方法是在以太帧的报文头中加入VLAN Tag,让所有广播只针对具有相同VLAN Tag的设备生效。这样既缩小了广播域，也附带提高了安全性和可管理性，因为两个VLAN之间不能直接通信。如果确有通信的需要，就必须通过三层设备来进行。


然而VLAN有两个明显的缺陷，第一个缺陷在于VLAN Tag的设计,VLAN Tag只预留了32bits的存储空间,其中VLAN ID只能有4096种取值。当云计算数据中心出现后，即使不考虑虚拟化的需求，单是需要分配 IP 的物理设备都有可能数以万计甚至数以十万计，这样 4096 个 VLAN 肯定是不够用的。后来 IEEE 的工程师们又提出802.1AQ 规范 力图补救这个缺陷，大致思路是给以太帧连续打上两个 VLAN Tag，每个 Tag 里仍然只有 12 Bits 的 VLAN ID，但两个加起来就可以存储 224=16,777,216 个不同的 VLAN ID 了，由于两个 VLAN Tag 并排放在报文头上，802.1AQ 规范还有了个 QinQ（802.1Q in 802.1Q）的昵称别名。


QinQ 是 2011 年推出的规范，但是直到现在都并没有特别普及，除了需要设备支持外，它还解决不了 VLAN 的第二个缺陷：跨数据中心传递。VLAN 本身是为二层网络所设计的，但是在两个独立数据中心之间，信息只能够通过三层网络传递，由于云计算的发展普及，大型分布式系统已不局限于单个数据中心，完全有跨数据中心运作的可能性，此时如何让 VLAN Tag 在两个数据中心间传递又成了不得不考虑的麻烦事。

为了统一解决以上两个问题，IETF 定义了 VXLAN 规范，这是三层虚拟化网络 （Network Virtualization over Layer 3，NVO3）的标准技术规范之一，是一种典型的 Overlay 网络。VXLAN 采用 L2 over L4 （MAC in UDP）的报文封装模式，把原本在二层传输的以太帧放到四层 UDP 协议的报文体内，同时加入了自己定义的 VXLAN Header。在 VXLAN Header 里直接就有 24 Bits 的 VLAN ID，同样可以存储 1677 万个不同的取值，VXLAN 让二层网络得以在三层范围内进行扩展，不再受数据中心间传输的限制。


VXLAN 对网络基础设施的要求很低，不需要专门的硬件提供的特别支持，只要三层可达的网络就能部署 VXLAN。VXLAN 网络的每个边缘入口上布置有一个 VTEP（VXLAN Tunnel Endpoints）设备，它既可以是物理设备，也可以是虚拟化设备，负责 VXLAN 协议报文的封包和解包。互联网号码分配局 （Internet Assigned Numbers Authority，IANA）专门分配了 4789 作为 VTEP 设备的 UDP 端口（以前 Linux VXLAN 用的默认端口是 8472，目前这两个端口在许多场景中仍有并存的情况）。

从linux kernel3.7版本起，linux系统就开始支持VXLAN。到了3.12版本，linux对VXLAN的支持已达到完全完备的程度，能够处理单播和组播，能够运行于IPv4和IPv6之上，一台linux主机经过简单配置之后，便可以把linux bridge作为VTEP设备使用。

VXLAN带来了很高的灵活性、扩展性和可管理性，同一套物理网络中可以任意创建多个VXLAN网络,每个VXLAN种接入的设备都仿佛是在一个完全独立的二层局域网中一样，不会受到外部广播的干扰，也很难遭受外部的攻击，这使得VXLAN能够良好地匹配分布式系统的弹性需求。不过，VXLAN也带来额外的复杂度和性能开销

+ 传输效率下降,新增加的报文头就有50bytes(VXLAN报文头占8bytes,UDP报文头占8bytes,IP报文头占20bytes,以太帧的MAC头占14bytes)，而原本只需要 14 Bytes 而已，而且现在这 14 Bytes 的消耗也还在，被封到了最里面的以太帧中。以太网的MTU 是 1500 Bytes，如果是传输大量数据，额外损耗 50 Bytes 并不算很高的成本，但如果传输的数据本来就只有几个 Bytes 的话，那传输消耗在报文头上的成本就很高昂了。
+ 传输性能的下降，每个 VXLAN 报文的封包和解包操作都属于额外的处理过程，尤其是用软件来实现的 VTEP，额外的运算资源消耗有时候会成为不可忽略的性能影响因素

## 副本网卡: MACVLAN


前文提到过VLAN之间是完全两层隔离的，不存在重合的广播域，因此要通信就只能通过三层设备，最简单的三层通信就是靠单臂路由了

假设位于VLAN-A中的主机A1希望将数据包发送给VLAN-B中的主机B2,由于A、B两个VLAN之间二层链路不同，因此引入了单臂路由，单臂路由不属于任何VLAN，它与交换机之间的链路允许任何VLAN ID的数据包通过，这种接口被称为TRUNK。这样,A1要和B2通信，A1就将数据包先发送给路由(只需把路由设置为网关即可做到),然后路由根据数据包上的IP地址得到B2的位置，去掉VLAN-A的VLAN Tag,改为VLAN-B的VLAN Tag重新封装数据后发回给交换机，交换机收到后就可以顺利转发给B2了。这个过程并没有什么复杂的地方，但有一个问题是,路由器该设置怎样的IP?由于A1、B2各自处于独立的网段上，它们又各自将同一个路由作为网管使用，这就要求路由器必须具备192.168.1.0/24 和 192.168.2.0/24 的 IP 地址。如果真的就只有 VLAN-A、VLAN-B 两个 VLAN，那把路由器上的两个接口分别设置不同的 IP 地址，然后用两条网线分别连接到交换机上也勉强算是一个解决办法，但 VLAN 最多支持 4096 个 VLAN，如果要接四千多条网线就太离谱了。为了解决这个问题，802.1Q 规范中专门定义了子接口（Sub-Interface）的概念，其作用是允许在同一张物理网卡上，针对不同的 VLAN 绑定不同的 IP 地址。

MACVLAN 借用了 VLAN 子接口的思路，并且在这个基础上更进一步，不仅允许对同一个网卡设置多个 IP 地址，还允许对同一张网卡上设置多个 MAC 地址，这也是 MACVLAN 名字的由来。原本 MAC 地址是网卡接口的“身份证”，应该是严格的一对一关系，而 MACVLAN 打破这层关系，方法是在物理设备之上、网络栈之下生成多个虚拟的 Device，每个 Device 都有一个 MAC 地址，新增 Device 的操作本质上相当于在系统内核中注册了一个收发特定数据包的回调函数，每个回调函数都能对一个 MAC 地址的数据包进行响应，当物理设备收到数据包时，会先根据 MAC 地址进行一次判断，确定交给哪个 Device 来处理


用 MACVLAN 技术虚拟出来的副本网卡，在功能上和真实的网卡是完全对等的，此时真正的物理网卡实际上确实承担着类似交换机的职责，收到数据包后，根据目标 MAC 地址判断这个包应转发给哪块副本网卡处理，由同一块物理网卡虚拟出来的副本网卡，天然处于同一个 VLAN 之中，可以直接二层通信，不需要将流量转发到外部网络。


与 Linux Bridge 相比，这种以网卡模拟交换机的方法在目标上并没有本质的不同，但 MACVLAN 在内部实现上要比 Linux Bridge 轻量得多。从数据流来看，副本网卡的通信只比物理网卡多了一次判断而已，能获得很高的网络通信性能；从操作步骤来看，由于 MAC 地址是静态的，所以 MACVLAN 不需要像 Linux Bridge 那样考虑 MAC 地址学习、STP 协议等复杂的算法，这进一步突出了 MACVLAN 的性能优势。


> 关于linux bridge和MACVLAN的首先最主要的区别为，MACVLAN是多张虚拟网卡的实现方案,而linux bridge提供了二层网络之间的转发方案。linux bridge与传统的交换机最大的不同是linux bridge还能与特定的三层网络中的IP进行特定操作


## 容器间通信


**所有的容器网络通信问题，都可以归结为本地主机内部的多个容器之间、本地主机与内部容器之间和跨越不同主机的多个容器之间的通信问题**