# linux network virtual

linux目前提供的八种名称空间里，网络名称空间无疑是隔离内容最多的一种，它为名称空间内所有进程提供了全套的网络设施，包括独立的设备界面、路由表、ARP表、IP地址表、iptables/ebtables规则、协议栈等等。虚拟化容器是以linux名称空间的隔离性为基础来实现的,那解决隔离的容器之间、容器与宿主机之间、乃至跨物理网络的不同容器间通信问题的责任，很自然也落在了 Linux 网络虚拟化技术的肩上。


## 网络通信模型

整体来看，linux系统的通信过程无论是按照理论上的OSI七层模型，还是以实际上的TCP/IP四层模型来解构，都明显地呈现出"逐层调用，逐层封装"的特点，这种逐层处理的方式与栈结构类似，因此它通常被称为linux网络协议栈。

网络栈(应用层以下)都位于系统内核之中，采用这种设计，主要是从数据安全隔离的角度出发考虑的。由内核去处理网络报文的收发，无疑会有更高的执行开销，譬如数据在内核态和用户态之间来回拷贝的额外成本，因此会损失一些性能，但是能够保证应用程序无法窃听或者去伪造另一个应用程序的通信内容。针对特别关注收发性能的应用场景，也有直接在用户空间中实现全套协议栈的旁路方案，譬如开源的Netmap以及Intel的DPDK,都能做到零拷贝收发网络数据包。

+ socket: 应用层的程序是通过socket编程接口来和内核空间的网络协议栈通信的。linux socket是从BSD socket发展而来的，现在socket已经不局限于某个操作系统的专属功能，成为各大主流操作系统共同支持的通用网络编程接口,是网络应用程序实际上的交互基础。应用程序通过读写收、发缓冲区来与socket进行交互，在unix和linux系统中，处于一切皆是文件的设计哲学，对socket操作被实现为对文件系统的读写访问操作,通过文件描述符来进行。
+ TCP/UDP:传输层协议族里面最重要的协议就是传输控制协议和用户数据报协议两种，它们是在linux内核中被直接支持的协议，此外还有流控制传输协议(SCTP)、数据报拥塞控制协议(DCCP)等等。不同的协议处理流程大致是一样的，只是把封装的报文以及头、尾部信息会有所不同，这里以 TCP 协议为例，内核发现 Socket 的发送缓冲区中有新的数据被拷贝进来后，会把数据封装为 TCP Segment 报文，常见网络协议的报文基本上都是由报文头（Header）和报文体（Body，也叫荷载“Payload”）两部分组成。系统内核将缓冲区中用户要发送出去的数据作为报文体，然后把传输层中的必要控制信息，譬如代表哪个程序发、由哪个程序收的源、目标端口号，用于保证可靠通信（重发与控制顺序）的序列号、用于校验信息是否在传输中出现损失的校验和（Check Sum）等信息封装入报文头中。
+ IP:网络层协议最主要的就是网际协议(IP),其他还有因特网组管理协议(IGMP)、大量的路由协议(EGP、NHRP、OSRF、IGRP)等等。
+ Device:网络设备是网络访问层中面向系统一侧的接口，这里所说的设备与物理硬件设备并不是同一个概念，Device只是一种向操作系统端开放的接口，其背后既可能是真实的物理硬件，也有可能是某段具有特定功能的程序代码,譬如即使不存在物理网卡，也依然可以存在回环设备。许多网络抓包工具便是在此处工作的。Device 主要的作用是抽象出统一的界面，让程序代码去选择或影响收发包出入口，譬如决定数据应该从哪块网卡设备发送出去；还有就是准备好网卡驱动工作所需的数据，譬如来自上一层的 IP 数据包、下一跳 （Next Hop）的 MAC 地址（这个地址是通过ARP Request 得到的）等等。
+ Driver:网卡驱动程序是网络访问层中面向硬件一侧的接口，网卡驱动程序会通过DMA将主存中的待发送数据包复制到驱动内部的缓冲区之内。数据被复制的同时，也会将上层提供的IP数据包、下一条MAC地址这些信息，加上网卡的MAC地址、VLAN Tag等信息一并封装成为以太帧,并自动计算校验和。对于需要确认重发的信息，如果没有收到接收者的确认响应，重发的处理也是在这里自动完成的。


## 干预网络通信

网络协议栈的处理是一套相对固定和封闭的流程，整套处理过程中，除了在网络设备这层能看到一点点程序以设备的形式介入处理的空间外，其他过程似乎就没有什么可供程序插手的余地了。然而在linux kernel 2.4版开始，内核开放了一套通用的、可供代码干预数据在协议栈中流转的过滤器框架。这套名为Netfilter框架是linux防火墙和网络的主要维护者提出的，它围绕网络层(IP协议)的周围，埋下了五个钩子,每当有数据包流到网络层，经过这层钩子时，就会自动出发由内核模块注册在这里的回调函数，程序代码就能够通过回调来干预linux网络通信。

+ PREROUTING:来自设备的数据包进入协议栈后立即触发此钩子。PREROUTING钩子在进入IP路由之前触发，这意味着只要接收到的数据包，无论是否真的发往本机，都会触发此钩子。一般用于目标网络地址转换。
+ INPUT:报文经过IP路由后，如果确定是发往本机的，就会触发此钩子，一般用于加工发往本地进程的数据包
+ FORWARD:报文经过IP路由后，如果确定不是发往本机的，就会触发此钩子，一般用于处理转发到其他机器的数据包
+ OUTPUT: 从本机程序发出的数据包。在经过IP路由前，就会触发此钩子，一般用于加工本地进程的输出数据包
+ POSTROUTING:从本机网卡出去的数据包,无论是本机的程序发出的，还是由本机转发给其他机器的，都会触发此钩子，一般用于源网络地址转换。

Netfilter允许在同一钩子处注册多个回调函数，因此向钩子注册回调函数时必须提供明确的优先级，以便触发时能按照优先级从高到低进行激活。由于回调函数会存在很多个，看起来就像挂在同一个钩子的一串链条，因此钩子触发的回调函数集合就被称为"回调链"。这个名字导致了后续基于 Netfilter 设计的 Xtables 系工具，如稍后介绍的 iptables 均有使用到“链”（Chain）的概念。**虽然现在看来 Netfilter 只是一些简单的事件回调机制而已，然而这样一套简单的设计，却成为了整座 Linux 网络大厦的核心基石，Linux 系统提供的许多网络能力，如数据包过滤、封包处理（设置标志位、修改 TTL 等）、地址伪装、网络地址转换、透明代理、访问控制、基于协议类型的连接跟踪，带宽限速，等等，都是在 Netfilter 基础之上实现的。**



iptables被称为linux系统的自带防火墙，然而iptables实际能做的事情已经远远超出防火墙的范畴。**iptables的设计意图是因为Netfilter的钩子回调虽然很强大，但是需要通过程序编码才能够使用，并不适合系统管理员日常运维，而它的价值便是以配置去实现原本用NetFilter编码才能做到的事情。iptables先把用户常用的管理意图总结成具体的行为预先准备好，然后在满足条件时自动激活行为。**



这些行为本来能够被挂载到Netfilter钩子的回调链上，但iptables又进行了一层额外抽象，不是把行为与链直接挂钩，而是根据这些底层操作的目的，先总结出更高层次的规则。举个例子，假设你挂载规则目的是为了实现网络地址转换（NAT），那就应该对符合某种特征的流量（譬如来源于某个网段、从某张网卡发送出去）、在某个钩子上（譬如做 SNAT 通常在 POSTROUTING，做 DNAT 通常在 PREROUTING）进行 MASQUERADE 行为，这样具有相同目的的规则，就应该放到一起才便于管理，由此便形成“规则表”的概念。iptables 内置了五张不可扩展的规则表（其中 security 表并不常用，很多资料只计算了前四张表），如下所列：

1. raw表:用于去除数据包上的连接追踪机制
2. mangle表:用于修改数据包的报文头信息，如服务类型、生存周期以及为数据包设置Mark标记，典型的应用是链路的服务质量管理
3. nat表，用于修改数据包的源或者目的地址等信息，典型的应用是网络地址转换
4. filter表:用于对数据包进行过滤，控制到达某条链上的数据包是继续放行还是直接丢弃或拒绝，典型的应用是防火墙
5. security表: 用于在数据包上应用SELinux，这张表并不常用

iptables 不仅仅是 Linux 系统自带的一个网络工具，它在容器间通信中扮演相当重要的角色，譬如 Kubernetes 用来管理 Service 的 Endpoints 的核心组件 kube-proxy，就依赖 iptables 来完成 ClusterIP 到 Pod 的通信（也可以采用 IPVS，IPVS 同样是基于 Netfilter 的），这种通信的本质就是一种 NAT 访问。



## 虚拟化网络设备


虚拟化网络并不需要完全遵照物理网络的样子来设计，不过，由于已有大量现成的代码本来就是面向于物理存在的网络设备来编码实现的，也有处于方便理解和知识继承的方面的考虑，虚拟化网络与物理网络中的设备还是有相当高的相似性。

## 虚拟网卡: tun/tap


目前主流的虚拟网卡方案有tun/tap和veth两种，在时间上tun/tap出现得更早，它是一组通用的虚拟驱动程序包，里面包含了两个设备，分别是用于网络数据包处理的虚拟网卡驱动，以及用于内核空间与用户空间交互的字符设备(这里指的是/dev/net/tun)驱动。大概在2000年左右，solaris系统为了实现隧道协议开发了这套驱动,从linux kernel2.1版开始移植到linux内核中，当时是源码中的可选模块，2.4版之后发布的内核都会默认编译tun/tap驱动。

tun和tap是两个相对独立的虚拟网络设备，其中tap模拟了以太网设备，操作二层数据包(以太帧)、tun则模拟了网络层设备，操作三层数据包。**使用tun/tap设备的目的是实现把来自协议栈的数据包先交由某个打开了/dev/net/tun字符设备的用户进程处理后,再把数据包重新发回到链路中。**可以通俗地将它理解为**这块虚拟化网卡驱动一端连接着网络协议栈，另一端连接着用户态程序，而普通的网卡驱动则是一端连接着网络协议栈，另一端连接着物理网卡**。只要协议栈中地数据包能被用户态程序截获并加工处理,程序员就有足够的舞台空间去玩出各种花样，譬如数据压缩，流量加密，透明代理等功能都可以以此为基础来实现。

应用程序通过tun设备对外发送数据包后，tun设备如果发现另一端的字符设备已经被VPN程序打开(这就是一端连接着网络协议栈，另一端连接着用户态程序)，便会把数据包通过字符设备发送给VPN程序，VPN收到数据包，会修改后再重新封装为新报文，譬如数据包原本是发送给A地址的，VPN把整个包进行加密，然后作为报文体，封装到另一个发送给B地址的新数据包当中。这种将一个数据包套进另一个数据包中的处理方式被形象称为隧道(Tunneling),隧道技术是再物理网络中构筑逻辑网络的经典做法。而其中提到的加密，也有标准的协议可遵循，譬如IPSec协议。


使用tun/tap设备传输数据需要经过两次协议栈，不可避免地会有一定的性能损耗，如果条件允许,容器对容器的直接通信并不会把tun/tap作为首选方案，一般是基于veth来实现的。但是tun/tap没有veth那样要求设备成对出现、数据要原样传输的限制，数据包到用户态进程后，程序员就有完全掌控的权力，要进行哪些修改，要发送到什么地方，都可以编写代码去实现，因此 tun/tap 方案比起 veth 方案有更广泛的适用范围。


## 虚拟网卡: veth

veth 是另一种主流的虚拟网卡方案，在 Linux Kernel 2.6 版本，Linux 开始支持网络名空间隔离的同时，也提供了专门的虚拟以太网（Virtual Ethernet，习惯简写做 veth）让两个隔离的网络名称空间之间可以互相通信。直接把 veth 比喻成是虚拟网卡其实并不十分准确，如果要和物理设备类比，它应该相当于由交叉网线 连接的一对物理网卡。


veth实际上不是一个设备，而是一对设备，因此也常被称作veth pair。要使用veth，必须在两个独立的网络名称空间中才有意义，因为veth pair是一端连接协议栈，另一端彼此相连的，在veth设备的其中一端输入数据，这些数据就会从设备的另一端原样不变地流出

由于两个容器之间采用 veth 通信不需要反复多次经过网络协议栈，这让 veth 比起 tap/tun 具有更好的性能，也让 veth pair 的实现变的十分简单，内核中只用了几十行代码实现了一个数据复制函数就完成了 veth 的主体功能。veth 以模拟网卡直连的方式很好地解决了两个容器之间的通信问题，然而对多个容器间通信，如果仍然单纯只用 veth pair 的话，事情就会变得非常麻烦，让每个容器都为与它通信的其他容器建立一对专用的 veth pair 并不实际，这时就迫切需要有一台虚拟化的交换机来解决多容器之间的通信问题了。


## 交换机 linux bridge

既然有了虚拟网卡，很自然也联想到让网卡接入到交换机里，实现多个容器间的相互连接。linux bridge便是linux系统下虚拟化交换机。虽然他以网桥而不是交换机命名，然后使用过程中，linux bridge的目的看起来就像交换机,功能使用起来像交换机、程序实现起来也像交换机，实际就是一台虚拟交换机。

linux bridge是在linux kernel 202版本开始提供的二层转发工具，由brctl命令创建和管理。linux bridge创建之后，便能够接入任何位于二层的网络设备，无论是真实的物理设备或者是虚拟的设备，都能与linux bridge配合工作，当有二层数据包(以太帧)从网卡进入linux bridge，它将根据数据包的类型和目标MAC地址，按如下规则转发处理:


+ 如果数据包是广播帧，转发给所有接入网桥的设备
+ 如果数据包是单播帧:

       + 且MAC地址在地址转发表中不存在，则泛洪给所有接入网桥的设备，并将响应设备的接口与MAC地址学习，到自己的MAC地址转发表中。
       + 且MAC地址在转发表中已经存在，则直接转发到地址表中指定的设备
+ 如果数据包是此前转发过的，又会重新发挥bridge，说明冗余链路产生了回环。由于以太帧不像IP报文那样有TTL来约束，因此一旦出现环路，如果没有额外措施来处理的话就会永不停歇地转发下去。对于这种数据包就需要交换机实现生成树协议(STP)来交换拓扑信息，生成唯一拓扑链路以切断环路。